import ast
import json
import os
from io import StringIO

import numpy as np
import pandas as pd
from autogluon.multimodal import MultiModalPredictor
from constants import (
    ALLOWED_INPUT_FORMATS,
    ALLOWED_OUTPUT_FORMATS,
    BRACKET_FORMATTER,
    COLUMN_NAMES,
    COMMA_DELIMITER,
    CSV_FORMAT,
    JSON_FORMAT,
    LABELS,
    NEW_LINE_CHARACTER,
    NUM_GPU,
    PREDICTED_LABEL,
    PREDICTIONS,
    PROBABILITIES,
    SAGEMAKER_INFERENCE_OUTPUT,
)
from utils import infer_type_and_cast_value

INFERENCE_OUTPUT = (
    infer_type_and_cast_value(os.getenv(SAGEMAKER_INFERENCE_OUTPUT))
    if SAGEMAKER_INFERENCE_OUTPUT in os.environ
    else [PREDICTED_LABEL]
)
NUM_GPUS = infer_type_and_cast_value(os.getenv(NUM_GPU))


def generate_single_csv_line_inference_selection(data):
    """Generate a single csv line response.

    :param data: list of output generated from the model
    :return: csv line for the predictions
    """
    for single_prediction in data:
        contents = (
            BRACKET_FORMATTER.format(single_prediction)
            if isinstance(single_prediction, list)
            else str(single_prediction)
        )
        yield contents


def model_fn(model_dir):
    """Load model from previously saved artifact.

    :param model_dir: local path to the model directory
    :return: loaded model
    """
    predictor = MultiModalPredictor.load(model_dir)
    if NUM_GPUS is not None:
        predictor._config.env.num_gpus = NUM_GPUS
    globals()[COLUMN_NAMES] = [
        key for key in predictor.column_types.keys() if key != predictor._label_column
    ]

    return predictor


def build_response(inference_output_list, output_content_type, predictions):
    """Build the inference response based on computed predictions.

    :param inference_output_list: list of keys to be included in the response
    :param output_content_type: content type of the output
    :param predictions: list of predictions
    :return:
    """
    if output_content_type.lower() == CSV_FORMAT:
        response = build_csv_response(inference_output_list, predictions)
    else:
        response = build_json_response(inference_output_list, output_content_type, predictions)
    return response


def build_json_response(inference_output_list, output_content_type, predictions):
    """Build the json response.

    :param inference_output_list: list of keys to be included in the response
    :param output_content_type: content type of the output
    :param predictions: list of predictions
    :return:
    """
    dict_lines = []
    for entry in zip(*predictions):
        result_dict = dict()
        for idx, key in enumerate(inference_output_list):
            try:
                result_dict[key] = ast.literal_eval(entry[idx])
            except ValueError:
                result_dict[key] = entry[idx]
        dict_lines.append(result_dict)
    if output_content_type.lower() == JSON_FORMAT:
        response = json.dumps(dict({PREDICTIONS: dict_lines}))
    else:
        response = NEW_LINE_CHARACTER.join([json.dumps(line) for line in dict_lines])
    return response


def build_csv_response(inference_output_list, predictions):
    """Build the CSV response.

    :param inference_output_list:  list of keys to be included in the response
    :param predictions: list of predictions
    :return:
    """
    csv_lines = []
    for entry in zip(*predictions):
        result_list = list()
        for idx, key in enumerate(inference_output_list):
            if key == PREDICTED_LABEL:
                result_list.append(entry[idx])
            else:
                if isinstance(ast.literal_eval(entry[idx]), list):
                    result_list.append('"{}"'.format(entry[idx]))
                else:
                    result_list.append(entry[idx])
        line = COMMA_DELIMITER.join(result_list)
        csv_lines.append(line)
    response = NEW_LINE_CHARACTER.join(csv_lines)
    return response


def transform_fn(model, request_body, input_content_type, output_content_type):
    """Transform function for serving inference requests.

    If INFERENCE_OUTPUT is provided, then the predictions are generated in the requested format and concatenated in the
    same order. Otherwise, prediction_labels are generated by default.

    :param model: loaded model
    :param request_body: request body
    :param input_content_type: content type of the input
    :param output_content_type: content type of the response
    :return: prediction response
    """
    if input_content_type.lower() not in ALLOWED_INPUT_FORMATS:
        raise Exception(
            f"{input_content_type} input content type not supported. Supported formats are {ALLOWED_INPUT_FORMATS}"
        )

    if output_content_type.lower() not in ALLOWED_OUTPUT_FORMATS:
        raise Exception(
            f"{output_content_type} output content type not supported. Supported formats are {ALLOWED_OUTPUT_FORMATS}"
        )

    buf = StringIO(request_body)
    data = pd.read_csv(buf, header=None)
    num_cols = len(data.columns)
    global_column_names = globals()[COLUMN_NAMES]
    if num_cols != len(global_column_names):
        raise Exception(
            f"Input data has {num_cols} columns while model expects {len(global_column_names)} {global_column_names}"
        )
    data.columns = global_column_names

    result = []
    inference_output_list = (
        INFERENCE_OUTPUT if isinstance(INFERENCE_OUTPUT, list) else [INFERENCE_OUTPUT]
    )
    for output_type in inference_output_list:
        if output_type == PREDICTED_LABEL:
            prediction = model.predict(data)
        elif output_type == PROBABILITIES:
            predict_probs = model.predict_proba(data)
            prediction = predict_probs.to_numpy()
        elif output_type == LABELS:
            num_rows = len(data)
            class_labels_single = pd.DataFrame(model.class_labels).T
            prediction = pd.concat([class_labels_single] * num_rows).to_numpy().astype("str")
        else:
            predict_probabilities = model.predict_proba(data).to_numpy()
            prediction = np.max(predict_probabilities, axis=1)
        result.append(generate_single_csv_line_inference_selection(prediction.tolist()))

    response = build_response(inference_output_list, output_content_type, result)

    return response, output_content_type
