{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SageMaker Modell zu ONNX konvertieren\n",
        "\n",
        "Dieses Notebook konvertiert das SageMaker-Modell (PyTorch Lightning Checkpoint) zu ONNX.\n",
        "\n",
        "## Schritte:\n",
        "1. Installation der ben√∂tigten Pakete\n",
        "2. Laden des Checkpoints und der Konfiguration\n",
        "3. Erstellen des Modells und Laden der Gewichte\n",
        "4. Konvertierung zu ONNX\n",
        "5. Validierung des ONNX-Modells\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Installation der ben√∂tigten Pakete\n",
        "%pip install -q transformers torch optimum[onnxruntime] onnxruntime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Alle Pakete erfolgreich importiert\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers.onnx import export\n",
        "import onnxruntime as ort\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Alle Pakete erfolgreich importiert\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Pfade definieren\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pfade:\n",
            "  Checkpoint: ..\\modelsagemaker\\model.ckpt\n",
            "  Konfiguration: ..\\modelsagemaker\\hf_text\n",
            "  ONNX-Ausgabe: ..\\modelsagemaker\\model.onnx\n",
            "\n",
            "‚úÖ Alle ben√∂tigten Dateien gefunden\n"
          ]
        }
      ],
      "source": [
        "# Pfade\n",
        "SAGEMAKER_MODEL_PATH = Path(\"../modelsagemaker\")\n",
        "SAGEMAKER_CKPT_PATH = SAGEMAKER_MODEL_PATH / \"model.ckpt\"\n",
        "SAGEMAKER_HF_TEXT_PATH = SAGEMAKER_MODEL_PATH / \"hf_text\"\n",
        "SAGEMAKER_ONNX_PATH = SAGEMAKER_MODEL_PATH / \"model.onnx\"\n",
        "ASSETS_JSON_PATH = SAGEMAKER_MODEL_PATH / \"assets.json\"\n",
        "\n",
        "print(\"Pfade:\")\n",
        "print(f\"  Checkpoint: {SAGEMAKER_CKPT_PATH}\")\n",
        "print(f\"  Konfiguration: {SAGEMAKER_HF_TEXT_PATH}\")\n",
        "print(f\"  ONNX-Ausgabe: {SAGEMAKER_ONNX_PATH}\")\n",
        "\n",
        "# Pr√ºfe ob Dateien existieren\n",
        "if not SAGEMAKER_CKPT_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint nicht gefunden: {SAGEMAKER_CKPT_PATH}\")\n",
        "if not SAGEMAKER_HF_TEXT_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Konfiguration nicht gefunden: {SAGEMAKER_HF_TEXT_PATH}\")\n",
        "\n",
        "print(\"\\n‚úÖ Alle ben√∂tigten Dateien gefunden\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Konfiguration und Anzahl Labels laden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lade Konfiguration...\n",
            "‚úÖ Konfiguration geladen: deberta-v2\n",
            "Anzahl Labels aus assets.json: 3\n",
            "‚ö†Ô∏è  Warnung: config.num_labels (2) != assets.output_shape (3)\n",
            "Verwende: 3 (aus assets.json)\n",
            "\n",
            "‚úÖ Anzahl Labels: 3\n"
          ]
        }
      ],
      "source": [
        "# Lade Konfiguration\n",
        "print(\"Lade Konfiguration...\")\n",
        "config = AutoConfig.from_pretrained(str(SAGEMAKER_HF_TEXT_PATH))\n",
        "print(f\"‚úÖ Konfiguration geladen: {config.model_type}\")\n",
        "\n",
        "# Bestimme Anzahl Labels\n",
        "num_labels = 2  # Standard f√ºr Hate Speech Detection\n",
        "if ASSETS_JSON_PATH.exists():\n",
        "    with open(ASSETS_JSON_PATH, 'r') as f:\n",
        "        assets = json.load(f)\n",
        "        if 'output_shape' in assets and assets['output_shape']:\n",
        "            num_labels = assets['output_shape']\n",
        "            print(f\"Anzahl Labels aus assets.json: {num_labels}\")\n",
        "        else:\n",
        "            print(f\"Verwende Standard: {num_labels} Labels\")\n",
        "else:\n",
        "    print(f\"assets.json nicht gefunden, verwende Standard: {num_labels} Labels\")\n",
        "\n",
        "# Setze num_labels in der Konfiguration\n",
        "if not hasattr(config, 'num_labels') or config.num_labels is None:\n",
        "    config.num_labels = num_labels\n",
        "else:\n",
        "    # Verwende die Anzahl aus der Konfiguration, aber pr√ºfe Konsistenz\n",
        "    if config.num_labels != num_labels:\n",
        "        print(f\"‚ö†Ô∏è  Warnung: config.num_labels ({config.num_labels}) != assets.output_shape ({num_labels})\")\n",
        "        print(f\"Verwende: {num_labels} (aus assets.json)\")\n",
        "        config.num_labels = num_labels\n",
        "\n",
        "print(f\"\\n‚úÖ Anzahl Labels: {config.num_labels}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Modell erstellen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Erstelle Modell mit 3 Labels...\n",
            "‚úÖ Modell erstellt\n",
            "   Modell-Typ: DebertaV2ForSequenceClassification\n",
            "   Anzahl Parameter: 184,424,451\n"
          ]
        }
      ],
      "source": [
        "# Erstelle Modell mit der Konfiguration\n",
        "print(f\"Erstelle Modell mit {config.num_labels} Labels...\")\n",
        "model = AutoModelForSequenceClassification.from_config(config)\n",
        "print(f\"‚úÖ Modell erstellt\")\n",
        "print(f\"   Modell-Typ: {type(model).__name__}\")\n",
        "print(f\"   Anzahl Parameter: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gewichte aus Checkpoint laden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lade Checkpoint: ..\\modelsagemaker\\model.ckpt\n",
            "Dateigr√∂√üe: 0.68 GB\n",
            "\n",
            "Checkpoint-Struktur:\n",
            "  Keys: ['state_dict']...\n",
            "  state_dict gefunden mit 201 Keys\n",
            "\n",
            "Erste Keys (Beispiele):\n",
            "  model.model.embeddings.position_ids\n",
            "  model.model.embeddings.word_embeddings.weight\n",
            "  model.model.embeddings.LayerNorm.weight\n",
            "  model.model.embeddings.LayerNorm.bias\n",
            "  model.model.encoder.layer.0.attention.self.query_proj.weight\n"
          ]
        }
      ],
      "source": [
        "# Lade Checkpoint\n",
        "print(f\"Lade Checkpoint: {SAGEMAKER_CKPT_PATH}\")\n",
        "print(f\"Dateigr√∂√üe: {SAGEMAKER_CKPT_PATH.stat().st_size / (1024**3):.2f} GB\")\n",
        "\n",
        "checkpoint = torch.load(str(SAGEMAKER_CKPT_PATH), map_location='cpu')\n",
        "\n",
        "# Pr√ºfe Checkpoint-Struktur\n",
        "print(f\"\\nCheckpoint-Struktur:\")\n",
        "if isinstance(checkpoint, dict):\n",
        "    print(f\"  Keys: {list(checkpoint.keys())[:10]}...\")\n",
        "    \n",
        "    if 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "        print(f\"  state_dict gefunden mit {len(state_dict)} Keys\")\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "        print(f\"  Direktes state_dict mit {len(state_dict)} Keys\")\n",
        "else:\n",
        "    state_dict = checkpoint\n",
        "    print(f\"  Checkpoint ist direkt state_dict mit {len(state_dict)} Keys\")\n",
        "\n",
        "# Zeige erste Keys\n",
        "first_keys = list(state_dict.keys())[:5]\n",
        "print(f\"\\nErste Keys (Beispiele):\")\n",
        "for key in first_keys:\n",
        "    print(f\"  {key}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bereinige state_dict...\n",
            "Original Keys: 201\n",
            "Bereinigte Keys: 200\n",
            "\n",
            "üìã Mapping-Beispiele (erste 10):\n",
            "  model.model.embeddings.position_ids                          ‚Üí deberta.embeddings.word_embeddings.weight\n",
            "  model.model.embeddings.word_embeddings.weight                ‚Üí deberta.embeddings.LayerNorm.weight\n",
            "  model.model.embeddings.LayerNorm.weight                      ‚Üí deberta.embeddings.LayerNorm.bias\n",
            "  model.model.embeddings.LayerNorm.bias                        ‚Üí deberta.encoder.layer.0.attention.self.query_proj.weight\n",
            "  model.model.encoder.layer.0.attention.self.query_proj.weight ‚Üí deberta.encoder.layer.0.attention.self.query_proj.bias\n",
            "  model.model.encoder.layer.0.attention.self.query_proj.bias   ‚Üí deberta.encoder.layer.0.attention.self.key_proj.weight\n",
            "  model.model.encoder.layer.0.attention.self.key_proj.weight   ‚Üí deberta.encoder.layer.0.attention.self.key_proj.bias\n",
            "  model.model.encoder.layer.0.attention.self.key_proj.bias     ‚Üí deberta.encoder.layer.0.attention.self.value_proj.weight\n",
            "  model.model.encoder.layer.0.attention.self.value_proj.weight ‚Üí deberta.encoder.layer.0.attention.self.value_proj.bias\n",
            "  model.model.encoder.layer.0.attention.self.value_proj.bias   ‚Üí deberta.encoder.layer.0.attention.output.dense.weight\n",
            "\n",
            "‚úÖ Keys mit 'deberta.' Pr√§fix: 198\n",
            "   Erste 5: ['deberta.embeddings.word_embeddings.weight', 'deberta.embeddings.LayerNorm.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.0.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.bias']\n"
          ]
        }
      ],
      "source": [
        "# Bereinige state_dict: Entferne Pr√§fixe und mappe auf Deberta-Struktur\n",
        "print(\"\\nBereinige state_dict...\")\n",
        "cleaned_state_dict = {}\n",
        "\n",
        "for key, value in state_dict.items():\n",
        "    new_key = key\n",
        "    \n",
        "    # Entferne 'position_ids' (wird automatisch erstellt)\n",
        "    if 'position_ids' in new_key:\n",
        "        continue\n",
        "    \n",
        "    # Schritt 1: Entferne doppelte Pr√§fixe (model.model. -> model.)\n",
        "    if new_key.startswith('model.model.'):\n",
        "        new_key = new_key[12:]  # Entferne 'model.model.' (12 Zeichen)\n",
        "    elif new_key.startswith('model.'):\n",
        "        new_key = new_key[6:]  # Entferne 'model.' (6 Zeichen)\n",
        "    \n",
        "    # Schritt 2: Entferne weitere Pr√§fixe\n",
        "    for prefix in ['module.', 'backbone.']:\n",
        "        if new_key.startswith(prefix):\n",
        "            new_key = new_key[len(prefix):]\n",
        "            break\n",
        "    \n",
        "    # Schritt 3: WICHTIG - F√ºr DebertaV2ForSequenceClassification muss 'deberta.' Pr√§fix hinzugef√ºgt werden\n",
        "    # f√ºr Embedding/Encoder Keys, aber NICHT f√ºr classifier/pooler/head\n",
        "    if not new_key.startswith('deberta.') and not new_key.startswith('classifier') and not new_key.startswith('pooler') and not new_key.startswith('head'):\n",
        "        # Pr√ºfe ob es ein Embedding/Encoder Key ist (sollte deberta. haben)\n",
        "        if any(x in new_key for x in ['embeddings', 'encoder']):\n",
        "            new_key = 'deberta.' + new_key\n",
        "        # Pooler kann auch deberta. haben, aber pr√ºfe zuerst\n",
        "        elif new_key.startswith('pooler.') and not new_key.startswith('deberta.pooler.'):\n",
        "            # Pooler kann direkt 'pooler.' haben oder 'deberta.pooler.'\n",
        "            pass  # Behalte pooler. wie es ist (wird separat behandelt)\n",
        "    \n",
        "    # Schritt 4: AutoGluon verwendet manchmal 'head' statt 'classifier'\n",
        "    # Mappe 'head' auf 'classifier'\n",
        "    if new_key.startswith('head.'):\n",
        "        new_key = 'classifier.' + new_key[5:]  # Ersetze 'head.' mit 'classifier.'\n",
        "    \n",
        "    cleaned_state_dict[new_key] = value\n",
        "\n",
        "print(f\"Original Keys: {len(state_dict)}\")\n",
        "print(f\"Bereinigte Keys: {len(cleaned_state_dict)}\")\n",
        "\n",
        "# Zeige Mapping-Beispiele\n",
        "print(f\"\\nüìã Mapping-Beispiele (erste 10):\")\n",
        "for i, (orig_key, new_key) in enumerate(list(zip(list(state_dict.keys())[:10], list(cleaned_state_dict.keys())[:10]))):\n",
        "    if orig_key != new_key:\n",
        "        print(f\"  {orig_key[:60]:60} ‚Üí {new_key[:60]}\")\n",
        "    else:\n",
        "        print(f\"  {orig_key[:60]:60} (unver√§ndert)\")\n",
        "\n",
        "# Pr√ºfe ob deberta. Pr√§fix korrekt hinzugef√ºgt wurde\n",
        "deberta_keys = [k for k in cleaned_state_dict.keys() if k.startswith('deberta.')]\n",
        "print(f\"\\n‚úÖ Keys mit 'deberta.' Pr√§fix: {len(deberta_keys)}\")\n",
        "if deberta_keys:\n",
        "    print(f\"   Erste 5: {deberta_keys[:5]}\")\n",
        "\n",
        "# Pr√ºfe ob noch 'model.' Keys √ºbrig sind\n",
        "model_keys = [k for k in cleaned_state_dict.keys() if k.startswith('model.')]\n",
        "if model_keys:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNUNG: Noch {len(model_keys)} Keys mit 'model.' Pr√§fix gefunden!\")\n",
        "    print(f\"   Erste 5: {model_keys[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lade Gewichte in Modell...\n",
            "\n",
            "‚úÖ Gewichte geladen\n",
            "‚ö†Ô∏è  Fehlende Keys: 2\n",
            "   Erste 10: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "   ‚ö†Ô∏è  KRITISCH: Fehlende Classifier/Pooler Keys: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "‚úÖ Keine unerwarteten Keys\n",
            "\n",
            "‚úÖ Modell bereit f√ºr ONNX-Export\n"
          ]
        }
      ],
      "source": [
        "# Lade Gewichte in das Modell\n",
        "print(\"\\nLade Gewichte in Modell...\")\n",
        "missing_keys, unexpected_keys = model.load_state_dict(cleaned_state_dict, strict=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Gewichte geladen\")\n",
        "if missing_keys:\n",
        "    print(f\"‚ö†Ô∏è  Fehlende Keys: {len(missing_keys)}\")\n",
        "    print(f\"   Erste 10: {missing_keys[:10]}\")\n",
        "    # Pr√ºfe ob kritische Keys fehlen\n",
        "    critical_missing = [k for k in missing_keys if 'classifier' in k or 'pooler' in k]\n",
        "    if critical_missing:\n",
        "        print(f\"   ‚ö†Ô∏è  KRITISCH: Fehlende Classifier/Pooler Keys: {critical_missing}\")\n",
        "    else:\n",
        "        print(f\"   ‚ÑπÔ∏è  Fehlende Keys sind wahrscheinlich nicht kritisch (Embeddings/Encoder)\")\n",
        "else:\n",
        "    print(f\"‚úÖ Keine fehlenden Keys\")\n",
        "\n",
        "if unexpected_keys:\n",
        "    print(f\"‚ö†Ô∏è  Unerwartete Keys: {len(unexpected_keys)}\")\n",
        "    print(f\"   Erste 10: {unexpected_keys[:10]}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Keine unerwarteten Keys\")\n",
        "\n",
        "# Setze Modell in Evaluationsmodus\n",
        "model.eval()\n",
        "print(f\"\\n‚úÖ Modell bereit f√ºr ONNX-Export\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tokenizer laden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lade Tokenizer...\n",
            "‚úÖ Tokenizer geladen\n",
            "   Vocab-Gr√∂√üe: 128001\n",
            "   Max-L√§nge: 1000000000000000019884624838656\n"
          ]
        }
      ],
      "source": [
        "# Lade Tokenizer\n",
        "print(\"Lade Tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(str(SAGEMAKER_HF_TEXT_PATH))\n",
        "print(f\"‚úÖ Tokenizer geladen\")\n",
        "print(f\"   Vocab-Gr√∂√üe: {len(tokenizer)}\")\n",
        "print(f\"   Max-L√§nge: {tokenizer.model_max_length}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Zu ONNX konvertieren\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "KONVERTIERUNG ZU ONNX\n",
            "============================================================\n",
            "\n",
            "Ausgabe-Pfad: ..\\modelsagemaker\\model.onnx\n",
            "\n",
            "Methode 1: Verwende optimum.onnxruntime...\n",
            "   Tempor√§res Verzeichnis: C:\\Users\\mirko\\AppData\\Local\\Temp\\tmpy2d6rs2v\n",
            "   Speichere Modell und Tokenizer...\n",
            "   Konvertiere zu ONNX mit optimum...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from 'C:\\Users\\mirko\\AppData\\Local\\Temp\\tmpy2d6rs2v' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "The tokenizer you are loading from 'C:\\Users\\mirko\\AppData\\Local\\Temp\\tmpy2d6rs2v' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "The tokenizer you are loading from 'C:\\Users\\mirko\\AppData\\Local\\Temp\\tmpy2d6rs2v' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "The tokenizer you are loading from 'C:\\Users\\mirko\\AppData\\Local\\Temp\\tmpy2d6rs2v' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "The tokenizer you are loading from 'C:\\Users\\mirko\\AppData\\Local\\Temp\\tmpy2d6rs2v' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
            "The tokenizer you are loading from 'C:\\Users\\mirko\\AppData\\Local\\Temp\\tmpy2d6rs2v' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Speichere ONNX-Modell...\n",
            "\n",
            "‚úÖ ONNX-Modell erfolgreich erstellt (mit optimum)!\n",
            "   Pfad: ..\\modelsagemaker\\model.onnx\n",
            "   Gr√∂√üe: 704.39 MB\n",
            "   Tempor√§res Verzeichnis gel√∂scht\n"
          ]
        }
      ],
      "source": [
        "# Konvertiere zu ONNX\n",
        "print(\"=\"*60)\n",
        "print(\"KONVERTIERUNG ZU ONNX\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nAusgabe-Pfad: {SAGEMAKER_ONNX_PATH}\")\n",
        "\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    # Methode 1: Verwende optimum.onnxruntime (empfohlen)\n",
        "    try:\n",
        "        from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "        \n",
        "        print(\"\\nMethode 1: Verwende optimum.onnxruntime...\")\n",
        "        \n",
        "        # Erstelle tempor√§res Verzeichnis f√ºr Modell und Tokenizer\n",
        "        temp_model_dir = Path(tempfile.mkdtemp())\n",
        "        print(f\"   Tempor√§res Verzeichnis: {temp_model_dir}\")\n",
        "        \n",
        "        try:\n",
        "            # Speichere Modell und Tokenizer\n",
        "            print(\"   Speichere Modell und Tokenizer...\")\n",
        "            model.save_pretrained(str(temp_model_dir))\n",
        "            tokenizer.save_pretrained(str(temp_model_dir))\n",
        "            config.save_pretrained(str(temp_model_dir))\n",
        "            \n",
        "            # Konvertiere mit optimum\n",
        "            print(\"   Konvertiere zu ONNX mit optimum...\")\n",
        "            ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
        "                str(temp_model_dir),\n",
        "                export=True\n",
        "            )\n",
        "            \n",
        "            # Speichere das ONNX-Modell\n",
        "            print(\"   Speichere ONNX-Modell...\")\n",
        "            ort_model.save_pretrained(str(SAGEMAKER_MODEL_PATH))\n",
        "            \n",
        "            # Pr√ºfe ob ONNX-Datei erstellt wurde\n",
        "            if SAGEMAKER_ONNX_PATH.exists():\n",
        "                file_size = SAGEMAKER_ONNX_PATH.stat().st_size / (1024 * 1024)  # MB\n",
        "                print(f\"\\n‚úÖ ONNX-Modell erfolgreich erstellt (mit optimum)!\")\n",
        "                print(f\"   Pfad: {SAGEMAKER_ONNX_PATH}\")\n",
        "                print(f\"   Gr√∂√üe: {file_size:.2f} MB\")\n",
        "            else:\n",
        "                # Suche nach ONNX-Dateien im Verzeichnis\n",
        "                onnx_files = list(SAGEMAKER_MODEL_PATH.glob(\"*.onnx\"))\n",
        "                if onnx_files:\n",
        "                    # Verschiebe die erste gefundene ONNX-Datei\n",
        "                    found_onnx = onnx_files[0]\n",
        "                    if found_onnx != SAGEMAKER_ONNX_PATH:\n",
        "                        shutil.move(str(found_onnx), str(SAGEMAKER_ONNX_PATH))\n",
        "                    file_size = SAGEMAKER_ONNX_PATH.stat().st_size / (1024 * 1024)\n",
        "                    print(f\"\\n‚úÖ ONNX-Modell gefunden und verschoben: {SAGEMAKER_ONNX_PATH}\")\n",
        "                    print(f\"   Gr√∂√üe: {file_size:.2f} MB\")\n",
        "                else:\n",
        "                    raise FileNotFoundError(\"ONNX-Datei wurde nicht erstellt\")\n",
        "        finally:\n",
        "            # Aufr√§umen: L√∂sche tempor√§res Verzeichnis\n",
        "            if temp_model_dir.exists():\n",
        "                shutil.rmtree(temp_model_dir)\n",
        "                print(f\"   Tempor√§res Verzeichnis gel√∂scht\")\n",
        "                \n",
        "    except Exception as e1:\n",
        "        print(f\"‚ö†Ô∏è  optimum-Methode fehlgeschlagen: {e1}\")\n",
        "        import traceback\n",
        "        print(f\"   Fehlerdetails:\")\n",
        "        traceback.print_exc()\n",
        "        \n",
        "        print(\"\\nMethode 2: Verwende transformers.onnx.export...\")\n",
        "        \n",
        "        # Alternative: Verwende die direkte export-Funktion\n",
        "        # Die Signatur ist: export(model, tokenizer, output, opset=12)\n",
        "        try:\n",
        "            export(\n",
        "                model,\n",
        "                tokenizer,\n",
        "                str(SAGEMAKER_ONNX_PATH),\n",
        "                opset=12\n",
        "            )\n",
        "            \n",
        "            if SAGEMAKER_ONNX_PATH.exists():\n",
        "                file_size = SAGEMAKER_ONNX_PATH.stat().st_size / (1024 * 1024)  # MB\n",
        "                print(f\"\\n‚úÖ ONNX-Modell erfolgreich erstellt (mit transformers.onnx)!\")\n",
        "                print(f\"   Pfad: {SAGEMAKER_ONNX_PATH}\")\n",
        "                print(f\"   Gr√∂√üe: {file_size:.2f} MB\")\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"ONNX-Datei wurde nicht erstellt: {SAGEMAKER_ONNX_PATH}\")\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ö†Ô∏è  transformers.onnx.export fehlgeschlagen: {e2}\")\n",
        "            print(\"\\nMethode 3: Verwende torch.onnx.export (direkt)...\")\n",
        "            \n",
        "            # Fallback: Verwende torch.onnx.export direkt\n",
        "            model.eval()\n",
        "            \n",
        "            # Erstelle Dummy-Input\n",
        "            dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, 128))\n",
        "            dummy_attention_mask = torch.ones(1, 128, dtype=torch.long)\n",
        "            \n",
        "            # Pr√ºfe ob token_type_ids ben√∂tigt wird\n",
        "            try:\n",
        "                # Versuche mit token_type_ids\n",
        "                dummy_token_type_ids = torch.zeros(1, 128, dtype=torch.long)\n",
        "                dummy_input = (dummy_input_ids, dummy_attention_mask, dummy_token_type_ids)\n",
        "            except:\n",
        "                dummy_input = (dummy_input_ids, dummy_attention_mask)\n",
        "            \n",
        "            # Exportiere mit torch.onnx\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                dummy_input,\n",
        "                str(SAGEMAKER_ONNX_PATH),\n",
        "                export_params=True,\n",
        "                opset_version=12,\n",
        "                do_constant_folding=True,\n",
        "                input_names=['input_ids', 'attention_mask'],\n",
        "                output_names=['logits'],\n",
        "                dynamic_axes={\n",
        "                    'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
        "                    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
        "                    'logits': {0: 'batch_size'}\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            if SAGEMAKER_ONNX_PATH.exists():\n",
        "                file_size = SAGEMAKER_ONNX_PATH.stat().st_size / (1024 * 1024)  # MB\n",
        "                print(f\"\\n‚úÖ ONNX-Modell erfolgreich erstellt (mit torch.onnx)!\")\n",
        "                print(f\"   Pfad: {SAGEMAKER_ONNX_PATH}\")\n",
        "                print(f\"   Gr√∂√üe: {file_size:.2f} MB\")\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"ONNX-Datei wurde nicht erstellt: {SAGEMAKER_ONNX_PATH}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Fehler bei ONNX-Export: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nüí° L√∂sung:\")\n",
        "    print(\"1. Stellen Sie sicher, dass 'optimum[onnxruntime]' installiert ist\")\n",
        "    print(\"2. Starten Sie den Kernel neu nach der Installation\")\n",
        "    print(\"3. Versuchen Sie es erneut\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ONNX-Modell validieren\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Direktes Testen des PyTorch-Modells (ohne ONNX)\n",
        "\n",
        "Teste das Modell direkt, um zu pr√ºfen ob die Gewichte korrekt geladen wurden.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DIREKTER TEST DES PYTORCH-MODELLS\n",
            "============================================================\n",
            "\n",
            "Test-Text: Das ist ein Test-Text f√ºr Hate Speech Detection.\n",
            "\n",
            "Tokenisierte Eingabe:\n",
            "  input_ids Shape: torch.Size([1, 14])\n",
            "  attention_mask Shape: torch.Size([1, 14])\n",
            "\n",
            "‚úÖ Inferenz erfolgreich!\n",
            "   Logits Shape: torch.Size([1, 3])\n",
            "   Logits: [0.4112989604473114, -0.3233424723148346, -0.10449069738388062]\n",
            "   Wahrscheinlichkeiten: [0.48153170943260193, 0.23097988963127136, 0.2874883711338043]\n",
            "   Vorhergesagte Klasse: 0\n",
            "\n",
            "‚ö†Ô∏è  WARNUNG: Maximale Wahrscheinlichkeit ist sehr niedrig (0.4815)\n",
            "   Das deutet darauf hin, dass die Gewichte m√∂glicherweise nicht korrekt geladen wurden.\n",
            "\n",
            "============================================================\n",
            "TEST MIT MEHREREN BEISPIELEN\n",
            "============================================================\n",
            "\n",
            "1. Text: Das ist ein normaler Text ohne Hass....\n",
            "   Vorhergesagte Klasse: 0\n",
            "   Wahrscheinlichkeiten: ['0.5153', '0.2112', '0.2735']\n",
            "   Max. Wahrscheinlichkeit: 0.5153\n",
            "\n",
            "2. Text: Ich hasse dich und w√ºnsche dir den Tod!...\n",
            "   Vorhergesagte Klasse: 0\n",
            "   Wahrscheinlichkeiten: ['0.3825', '0.2603', '0.3572']\n",
            "   Max. Wahrscheinlichkeit: 0.3825\n",
            "\n",
            "3. Text: Guten Tag, wie geht es Ihnen heute?...\n",
            "   Vorhergesagte Klasse: 0\n",
            "   Wahrscheinlichkeiten: ['0.5245', '0.2127', '0.2628']\n",
            "   Max. Wahrscheinlichkeit: 0.5245\n"
          ]
        }
      ],
      "source": [
        "# Teste das PyTorch-Modell direkt (ohne ONNX)\n",
        "print(\"=\"*60)\n",
        "print(\"DIREKTER TEST DES PYTORCH-MODELLS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test-Text\n",
        "test_text = \"Das ist ein Test-Text f√ºr Hate Speech Detection.\"\n",
        "\n",
        "# Tokenisiere\n",
        "print(f\"\\nTest-Text: {test_text}\")\n",
        "encoded = tokenizer(\n",
        "    test_text,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenisierte Eingabe:\")\n",
        "print(f\"  input_ids Shape: {encoded['input_ids'].shape}\")\n",
        "print(f\"  attention_mask Shape: {encoded['attention_mask'].shape}\")\n",
        "\n",
        "# F√ºhre Inferenz aus\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "print(f\"\\n‚úÖ Inferenz erfolgreich!\")\n",
        "print(f\"   Logits Shape: {logits.shape}\")\n",
        "print(f\"   Logits: {logits[0].tolist()}\")\n",
        "print(f\"   Wahrscheinlichkeiten: {probabilities[0].tolist()}\")\n",
        "print(f\"   Vorhergesagte Klasse: {predicted_class}\")\n",
        "\n",
        "# Pr√ºfe ob die Wahrscheinlichkeiten sinnvoll sind\n",
        "max_prob = probabilities[0].max().item()\n",
        "if max_prob < 0.5:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNUNG: Maximale Wahrscheinlichkeit ist sehr niedrig ({max_prob:.4f})\")\n",
        "    print(f\"   Das deutet darauf hin, dass die Gewichte m√∂glicherweise nicht korrekt geladen wurden.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Maximale Wahrscheinlichkeit: {max_prob:.4f} (sinnvoll)\")\n",
        "\n",
        "# Teste mit mehreren Beispielen\n",
        "test_texts = [\n",
        "    \"Das ist ein normaler Text ohne Hass.\",\n",
        "    \"Ich hasse dich und w√ºnsche dir den Tod!\",\n",
        "    \"Guten Tag, wie geht es Ihnen heute?\"\n",
        "]\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"TEST MIT MEHREREN BEISPIELEN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "        max_prob = probabilities[0].max().item()\n",
        "    \n",
        "    print(f\"\\n{i}. Text: {text[:50]}...\")\n",
        "    print(f\"   Vorhergesagte Klasse: {predicted_class}\")\n",
        "    print(f\"   Wahrscheinlichkeiten: {[f'{p:.4f}' for p in probabilities[0].tolist()]}\")\n",
        "    print(f\"   Max. Wahrscheinlichkeit: {max_prob:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "VALIDIERUNG DES ONNX-MODELLS\n",
            "============================================================\n",
            "\n",
            "‚úÖ ONNX-Modell erfolgreich geladen\n",
            "\n",
            "Inputs:\n",
            "  - input_ids: ['batch_size', 'sequence_length'] (tensor(int64))\n",
            "  - attention_mask: ['batch_size', 'sequence_length'] (tensor(int64))\n",
            "\n",
            "Outputs:\n",
            "  - logits: ['batch_size', 3] (tensor(float))\n",
            "\n",
            "Teste mit Beispieltext...\n",
            "‚úÖ Inferenz erfolgreich!\n",
            "   Logits Shape: (1, 3)\n",
            "   Logits: [ 0.5305716  -0.4137432  -0.17257568]\n",
            "   Wahrscheinlichkeiten: [0.5307938  0.20645012 0.2627561 ]\n",
            "   Vorhergesagte Klasse: 0\n"
          ]
        }
      ],
      "source": [
        "# Validiere ONNX-Modell\n",
        "print(\"=\"*60)\n",
        "print(\"VALIDIERUNG DES ONNX-MODELLS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if SAGEMAKER_ONNX_PATH.exists():\n",
        "    try:\n",
        "        # Lade ONNX Session\n",
        "        session = ort.InferenceSession(str(SAGEMAKER_ONNX_PATH))\n",
        "        \n",
        "        print(\"\\n‚úÖ ONNX-Modell erfolgreich geladen\")\n",
        "        print(f\"\\nInputs:\")\n",
        "        for inp in session.get_inputs():\n",
        "            print(f\"  - {inp.name}: {inp.shape} ({inp.type})\")\n",
        "        \n",
        "        print(f\"\\nOutputs:\")\n",
        "        for out in session.get_outputs():\n",
        "            print(f\"  - {out.name}: {out.shape} ({out.type})\")\n",
        "        \n",
        "        # Teste mit einem Beispiel\n",
        "        print(f\"\\nTeste mit Beispieltext...\")\n",
        "        test_text = \"Das ist ein Testtext f√ºr die Validierung.\"\n",
        "        \n",
        "        # Tokenisiere\n",
        "        encodings = tokenizer(\n",
        "            test_text,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"np\"\n",
        "        )\n",
        "        \n",
        "        # ONNX erwartet int64\n",
        "        inputs = {\n",
        "            \"input_ids\": encodings[\"input_ids\"].astype(np.int64),\n",
        "            \"attention_mask\": encodings[\"attention_mask\"].astype(np.int64)\n",
        "        }\n",
        "        \n",
        "        # Pr√ºfe welche Inputs das ONNX-Modell ben√∂tigt\n",
        "        required_inputs = [inp.name for inp in session.get_inputs()]\n",
        "        if \"token_type_ids\" in required_inputs:\n",
        "            # Modell ben√∂tigt token_type_ids\n",
        "            if \"token_type_ids\" in encodings:\n",
        "                inputs[\"token_type_ids\"] = encodings[\"token_type_ids\"].astype(np.int64)\n",
        "            else:\n",
        "                # Erstelle token_type_ids (alle 0 f√ºr Single-Sequence-Tasks)\n",
        "                token_type_ids = np.zeros_like(encodings[\"input_ids\"], dtype=np.int64)\n",
        "                inputs[\"token_type_ids\"] = token_type_ids\n",
        "            print(f\"   ‚ÑπÔ∏è  token_type_ids hinzugef√ºgt (Modell ben√∂tigt es)\")\n",
        "        \n",
        "        # F√ºhre Inferenz aus\n",
        "        outputs = session.run(None, inputs)\n",
        "        logits = outputs[0]\n",
        "        \n",
        "        print(f\"‚úÖ Inferenz erfolgreich!\")\n",
        "        print(f\"   Logits Shape: {logits.shape}\")\n",
        "        print(f\"   Logits: {logits[0]}\")\n",
        "        \n",
        "        # Softmax\n",
        "        probs = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
        "        probs = probs / np.sum(probs, axis=-1, keepdims=True)\n",
        "        print(f\"   Wahrscheinlichkeiten: {probs[0]}\")\n",
        "        print(f\"   Vorhergesagte Klasse: {np.argmax(probs[0])}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Fehler bei Validierung: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"\\n‚ùå ONNX-Datei existiert nicht\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ZUSAMMENFASSUNG\n",
            "============================================================\n",
            "\n",
            "‚úÖ Konvertierung erfolgreich abgeschlossen!\n",
            "\n",
            "ONNX-Modell:\n",
            "  Pfad: ..\\modelsagemaker\\model.onnx\n",
            "  Gr√∂√üe: 704.39 MB\n",
            "  Anzahl Labels: 3\n",
            "  Modell-Typ: deberta-v2\n",
            "\n",
            "Das ONNX-Modell kann jetzt im Test-Notebook verwendet werden.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ZUSAMMENFASSUNG\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if SAGEMAKER_ONNX_PATH.exists():\n",
        "    file_size = SAGEMAKER_ONNX_PATH.stat().st_size / (1024 * 1024)\n",
        "    print(f\"\\n‚úÖ Konvertierung erfolgreich abgeschlossen!\")\n",
        "    print(f\"\\nONNX-Modell:\")\n",
        "    print(f\"  Pfad: {SAGEMAKER_ONNX_PATH}\")\n",
        "    print(f\"  Gr√∂√üe: {file_size:.2f} MB\")\n",
        "    print(f\"  Anzahl Labels: {config.num_labels}\")\n",
        "    print(f\"  Modell-Typ: {config.model_type}\")\n",
        "    print(f\"\\nDas ONNX-Modell kann jetzt im Test-Notebook verwendet werden.\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Konvertierung fehlgeschlagen\")\n",
        "    print(f\"   Bitte pr√ºfen Sie die Fehlermeldungen oben.\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
